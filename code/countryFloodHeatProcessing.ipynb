{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os \n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import rasterio\n",
        "import os\n",
        "import pystac\n",
        "import pystac_client\n",
        "import planetary_computer\n",
        "import geopandas as gpd\n",
        "from rich.table import Table\n",
        "import rioxarray\n",
        "import requests\n",
        "from shapely.geometry import Polygon, shape, box\n",
        "from shapely.ops import transform\n",
        "from rich.table import Table\n",
        "from rioxarray.merge import merge_arrays\n",
        "from rasterio.plot import plotting_extent\n",
        "from rasterstats import zonal_stats\n",
        "from xrspatial.zonal import stats as zonal_stats\n",
        "from geocube.api.core import make_geocube\n",
        "import xarray\n",
        "from azureml.core import Workspace, Dataset\n",
        "import multiprocessing\n",
        "from functools import partial"
      ],
      "outputs": [],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1695936193200
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.getcwd()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 1,
          "data": {
            "text/plain": "'/mnt/batch/tasks/shared/LS_root/mounts/clusters/p18q1262/code/Users/p18q126'"
          },
          "metadata": {}
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1696006297770
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# azureml-core of version 1.0.72 or higher is required\n",
        "from azureml.core import Workspace, Dataset\n",
        "\n",
        "subscription_id = '6c748216-f035-4623-80c3-29e1c28c7a56'\n",
        "resource_group = 'geo_microsoft_general'\n",
        "workspace_name = 'pop_heat_flood'\n",
        "\n",
        "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
        "\n",
        "dataset = Dataset.get_by_name(workspace, name='population_data')\n",
        "dataset.download(target_path='./data', overwrite=True)"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1696006463412
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory = os.getcwd() + '/data'                                                                               # set directory\n",
        "heat_data = rasterio.open('global_mean_wbgt_30.tif', masked=True)                                               # load heat data               "
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695936198731
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for fileName in os.listdir(directory):\n",
        "        if fileName.endswith(\".tif\"):\n",
        "        \n",
        "            f = os.path.join(directory, fileName)\n",
        "        \n",
        "            countryCode = fileName[0:3]                                                                             # set country name\n",
        "\n",
        "            outFileName = directory + '/' + countryCode + '_heat_flood_extract.csv'                                 # do not overwrite files already processes\n",
        "            \n",
        "            if os.path.exists(outFileName):\n",
        "                print(countryCode + ' - file exists - skipping')\n",
        "                continue\n",
        "        \n",
        "            print(f\"{countryCode} - Started\") # STATUS CHECK\n",
        "\n",
        "            country_pop = rioxarray.open_rasterio(f, masked=True)                                                   # open individual pop file with filepath\n",
        "\n",
        "            country_box = country_pop.rio.bounds()                                                                  # find bounding box for country\n",
        "\n",
        "            country_pop.name = \"pop_data\"                                                                           # change column header to pop_data\n",
        "\n",
        "            country_pop_df = country_pop.squeeze().to_dataframe().reset_index()                                     # squeeze to data frame\n",
        "\n",
        "            geometry = gpd.points_from_xy(country_pop_df.x, country_pop_df.y)                                       # set geometry\n",
        "\n",
        "            country_pop_gdf = gpd.GeoDataFrame(country_pop_df, crs=country_pop.rio.crs, geometry=geometry)          # convert to geopandas df\n",
        "\n",
        "            country_pop_gdf.dropna(subset=['pop_data'], how='all', inplace=True)                                    # remove pixels with no people\n",
        "\n",
        "            country_pop_gdf.index = range(len(country_pop_gdf))\n",
        "            coords = [(x,y) for x, y in zip(country_pop_gdf.x, country_pop_gdf.y)]\n",
        "            country_pop_gdf['heat_value'] = [x[0] for x in heat_data.sample(coords)]                                # extract heat data\n",
        "\n",
        "            print(f\"{countryCode} - Heat Completed\") # STATUS CHECK\n",
        "                \n",
        "            catalog = pystac_client.Client.open(\n",
        "                \"https://planetarycomputer.microsoft.com/api/stac/v1\", \n",
        "                modifier=planetary_computer.sign_inplace,)                                                          # access flood data on PC\n",
        "\n",
        "            AOI = shape({ \"coordinates\": [[\n",
        "                [country_box[0],\n",
        "                 country_box[1]],\n",
        "                    \n",
        "                  [country_box[0],\n",
        "                  country_box[3]],\n",
        "                    \n",
        "                  [country_box[2],\n",
        "                  country_box[1]],\n",
        "                    \n",
        "                [country_box[2],\n",
        "                 country_box[3]]\n",
        "                   ]],\n",
        "                  \"type\": \"Polygon\"}).envelope                                                                        # use bounding box from before to get data for each country\n",
        "\n",
        "\n",
        "            jrc = catalog.search(collections=[\"jrc-gsw\"], intersects=AOI)\n",
        "            items = jrc.item_collection()                                                                           # Use STACAPI to access the collection (find the number of images for each bounding box)\n",
        "\n",
        "            n = len(items)\n",
        "            doubles = dict()\n",
        "                \n",
        "            for i in range(0, n):\n",
        "                item = items[i]\n",
        "                    \n",
        "                doubles[i] = rioxarray.open_rasterio(item.assets[\"occurrence\"].href, overview_level=4).squeeze()    # use this loop to load each image\n",
        "\n",
        "            flood_data = merge_arrays(dataarrays = list(doubles.values()), crs = \"EPSG:4326\")                       # merge each image that's been loaded\n",
        "\n",
        "            flood_data = flood_data.where(flood_data != 100)\n",
        "            flood_data = flood_data.where(flood_data != 255)                                                        # Remove values of 100, and values of 255)\n",
        "                \n",
        "            flood_data.rio.write_nodata(flood_data.rio.nodata, encoded=True, inplace=True)                          # update nodata value to show the data has been masked\n",
        "\n",
        "            country_pop_gdf_buff = country_pop_gdf.copy()\n",
        "            country_pop_gdf_buff[\"geometry\"] = country_pop_gdf.geometry.buffer(0.008)                               # create buffered polygon for each population point\n",
        "            country_pop_gdf_buff[\"id\"] = country_pop_gdf_buff.index + 1                                             # used to extract flood mean (also add index value here to merge back)\n",
        "\n",
        "            out_grid = make_geocube(\n",
        "                        vector_data= country_pop_gdf_buff,\n",
        "                        measurements=[\"id\"],\n",
        "                        like=flood_data, \n",
        "                        )                                                                                           # make geocube for zonal stats & ensure the data are on the same grid\n",
        "                    \n",
        "            out_grid[\"flood_data\"] = (flood_data.dims, flood_data.values, flood_data.attrs, flood_data.encoding)    # merge pop and flood together\n",
        "\n",
        "            grouped_flood_data = out_grid.drop(\"spatial_ref\").groupby(out_grid.id)\n",
        "            grid_mean = grouped_flood_data.mean().rename({\"flood_data\": \"flood_value\"})                             # extract mean flood within the polygon\n",
        "\n",
        "            print(f\"{countryCode} - Flood Completed\") # STATUS CHECK\n",
        "\n",
        "            zonal_stats = xarray.merge([grid_mean]).to_dataframe()                                                  # convert resulting file to dataframe\n",
        "\n",
        "            country_final_data = country_pop_gdf_buff.merge(zonal_stats, on='id')\n",
        "            country_final_data = pd.DataFrame(country_final_data.drop(columns=['geometry', 'band', 'spatial_ref'])) # merge back in to data with heat\n",
        "\n",
        "            country_final_data[\"country_iso\"] = countryCode                                                         # add country code as a final column for reference\n",
        "                \n",
        "            country_final_data.to_csv(outFileName, index = False)\n",
        "\n",
        "            print(f\"{countryCode} - Completed & Saved\") # STATUS CHECK"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "abw - file exists - skipping\nafg - file exists - skipping\nago - file exists - skipping\naia - file exists - skipping\nalb - file exists - skipping\nand - file exists - skipping\nare - file exists - skipping\narg - file exists - skipping\narm - file exists - skipping\nasm - file exists - skipping\natg - file exists - skipping\naus - Started\naus - Heat Completed\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_14933/2917271581.py:75: UserWarning: Geometry is in a geographic CRS. Results from 'buffer' are likely incorrect. Use 'GeoSeries.to_crs()' to re-project geometries to a projected CRS before this operation.\n\n  country_pop_gdf_buff[\"geometry\"] = country_pop_gdf.geometry.buffer(0.008)                               # create buffered polygon for each population point\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[4], line 87\u001b[0m\n\u001b[1;32m     84\u001b[0m out_grid[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflood_data\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m (flood_data\u001b[38;5;241m.\u001b[39mdims, flood_data\u001b[38;5;241m.\u001b[39mvalues, flood_data\u001b[38;5;241m.\u001b[39mattrs, flood_data\u001b[38;5;241m.\u001b[39mencoding)    \u001b[38;5;66;03m# merge pop and flood together\u001b[39;00m\n\u001b[1;32m     86\u001b[0m grouped_flood_data \u001b[38;5;241m=\u001b[39m out_grid\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspatial_ref\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mgroupby(out_grid\u001b[38;5;241m.\u001b[39mid)\n\u001b[0;32m---> 87\u001b[0m grid_mean \u001b[38;5;241m=\u001b[39m \u001b[43mgrouped_flood_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mrename({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflood_data\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflood_value\u001b[39m\u001b[38;5;124m\"\u001b[39m})                             \u001b[38;5;66;03m# extract mean flood within the polygon\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcountryCode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Flood Completed\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# STATUS CHECK\u001b[39;00m\n\u001b[1;32m     91\u001b[0m zonal_stats \u001b[38;5;241m=\u001b[39m xarray\u001b[38;5;241m.\u001b[39mmerge([grid_mean])\u001b[38;5;241m.\u001b[39mto_dataframe()                                                  \u001b[38;5;66;03m# convert resulting file to dataframe\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/_aggregations.py:2972\u001b[0m, in \u001b[0;36mDatasetGroupByAggregations.mean\u001b[0;34m(self, dim, skipna, keep_attrs, **kwargs)\u001b[0m\n\u001b[1;32m   2962\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flox_reduce(\n\u001b[1;32m   2963\u001b[0m         func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   2964\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2969\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   2970\u001b[0m     )\n\u001b[1;32m   2971\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2972\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mduck_array_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2974\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2978\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2979\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/groupby.py:1647\u001b[0m, in \u001b[0;36mDatasetGroupByBase.reduce\u001b[0;34m(self, func, dim, axis, keep_attrs, keepdims, shortcut, **kwargs)\u001b[0m\n\u001b[1;32m   1636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ds\u001b[38;5;241m.\u001b[39mreduce(\n\u001b[1;32m   1637\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   1638\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1642\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1643\u001b[0m     )\n\u001b[1;32m   1645\u001b[0m check_reduce_dims(dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdims)\n\u001b[0;32m-> 1647\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduce_dataset\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/groupby.py:1555\u001b[0m, in \u001b[0;36mDatasetGroupByBase.map\u001b[0;34m(self, func, args, shortcut, **kwargs)\u001b[0m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;66;03m# ignore shortcut if set (for now)\u001b[39;00m\n\u001b[1;32m   1554\u001b[0m applied \u001b[38;5;241m=\u001b[39m (func(ds, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_grouped())\n\u001b[0;32m-> 1555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_combine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapplied\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/groupby.py:1577\u001b[0m, in \u001b[0;36mDatasetGroupByBase._combine\u001b[0;34m(self, applied)\u001b[0m\n\u001b[1;32m   1575\u001b[0m applied_example, applied \u001b[38;5;241m=\u001b[39m peek_at(applied)\n\u001b[1;32m   1576\u001b[0m coord, dim, positions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_infer_concat_args(applied_example)\n\u001b[0;32m-> 1577\u001b[0m combined \u001b[38;5;241m=\u001b[39m \u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mapplied\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1578\u001b[0m (grouper,) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupers\n\u001b[1;32m   1579\u001b[0m combined \u001b[38;5;241m=\u001b[39m _maybe_reorder(combined, dim, positions, N\u001b[38;5;241m=\u001b[39mgrouper\u001b[38;5;241m.\u001b[39mgroup\u001b[38;5;241m.\u001b[39msize)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/concat.py:252\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _dataarray_concat(\n\u001b[1;32m    241\u001b[0m         objs,\n\u001b[1;32m    242\u001b[0m         dim\u001b[38;5;241m=\u001b[39mdim,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m         combine_attrs\u001b[38;5;241m=\u001b[39mcombine_attrs,\n\u001b[1;32m    250\u001b[0m     )\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(first_obj, Dataset):\n\u001b[0;32m--> 252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dataset_concat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdata_vars\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_vars\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoords\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoords\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpositions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfill_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfill_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcombine_attrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombine_attrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcan only concatenate xarray Dataset and DataArray \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjects, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(first_obj)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    267\u001b[0m     )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/concat.py:468\u001b[0m, in \u001b[0;36m_dataset_concat\u001b[0;34m(datasets, dim, data_vars, coords, compat, positions, fill_value, join, combine_attrs)\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataarray\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataArray\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mxarray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m--> 468\u001b[0m datasets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdatasets\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    470\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(dataset, Dataset) \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets):\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe elements in the input list need to be either all \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms or all \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataArray\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    473\u001b[0m     )\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/groupby.py:1554\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Apply a function to each Dataset in the group and concatenate them\u001b[39;00m\n\u001b[1;32m   1525\u001b[0m \u001b[38;5;124;03mtogether into a new Dataset.\u001b[39;00m\n\u001b[1;32m   1526\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[38;5;124;03m    The result of splitting, applying and combining this dataset.\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;66;03m# ignore shortcut if set (for now)\u001b[39;00m\n\u001b[0;32m-> 1554\u001b[0m applied \u001b[38;5;241m=\u001b[39m (func(ds, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;28;01mfor\u001b[39;00m ds \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter_grouped())\n\u001b[1;32m   1555\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine(applied)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/groupby.py:825\u001b[0m, in \u001b[0;36mGroupBy._iter_grouped\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over each element in this group\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indices \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_group_indices:\n\u001b[0;32m--> 825\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43misel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_group_dim\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/dataset.py:2897\u001b[0m, in \u001b[0;36mDataset.isel\u001b[0;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[1;32m   2894\u001b[0m dims: \u001b[38;5;28mdict\u001b[39m[Hashable, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2895\u001b[0m coord_names \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coord_names\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m-> 2897\u001b[0m indexes, index_variables \u001b[38;5;241m=\u001b[39m \u001b[43misel_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2899\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, var \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variables\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   2900\u001b[0m     \u001b[38;5;66;03m# preserve variable order\u001b[39;00m\n\u001b[1;32m   2901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m index_variables:\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/indexes.py:1796\u001b[0m, in \u001b[0;36misel_indexes\u001b[0;34m(indexes, indexers)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21misel_indexes\u001b[39m(\n\u001b[1;32m   1793\u001b[0m     indexes: Indexes[Index],\n\u001b[1;32m   1794\u001b[0m     indexers: Mapping[Any, Any],\n\u001b[1;32m   1795\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[\u001b[38;5;28mdict\u001b[39m[Hashable, Index], \u001b[38;5;28mdict\u001b[39m[Hashable, Variable]]:\n\u001b[0;32m-> 1796\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_apply_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindexes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43misel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/indexes.py:1780\u001b[0m, in \u001b[0;36m_apply_indexes\u001b[0;34m(indexes, args, func)\u001b[0m\n\u001b[1;32m   1778\u001b[0m index_args \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m index_dims}\n\u001b[1;32m   1779\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m index_args:\n\u001b[0;32m-> 1780\u001b[0m     new_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1781\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m new_index \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m         new_indexes\u001b[38;5;241m.\u001b[39mupdate({k: new_index \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m index_vars})\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/indexes.py:726\u001b[0m, in \u001b[0;36mPandasIndex.isel\u001b[0;34m(self, indexers)\u001b[0m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(indxr, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_scalar(indxr):\n\u001b[1;32m    723\u001b[0m     \u001b[38;5;66;03m# scalar indexer: drop index\u001b[39;00m\n\u001b[1;32m    724\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_replace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindxr\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/indexes.py:944\u001b[0m, in \u001b[0;36mPandasMultiIndex._replace\u001b[0;34m(self, index, dim, level_coords_dtype)\u001b[0m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level_coords_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    943\u001b[0m     level_coords_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevel_coords_dtype\n\u001b[0;32m--> 944\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel_coords_dtype\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/indexes.py:919\u001b[0m, in \u001b[0;36mPandasMultiIndex.__init__\u001b[0;34m(self, array, dim, level_coords_dtype)\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, array: Any, dim: Hashable, level_coords_dtype: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 919\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    921\u001b[0m     \u001b[38;5;66;03m# default index level names\u001b[39;00m\n\u001b[1;32m    922\u001b[0m     names \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/xarray/core/indexes.py:582\u001b[0m, in \u001b[0;36mPandasIndex.__init__\u001b[0;34m(self, array, dim, coord_dtype)\u001b[0m\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, array: Any, dim: Hashable, coord_dtype: Any \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    579\u001b[0m     \u001b[38;5;66;03m# make a shallow copy: cheap and because the index name may be updated\u001b[39;00m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;66;03m# here or in other constructors (cannot use pd.Index.rename as this\u001b[39;00m\n\u001b[1;32m    581\u001b[0m     \u001b[38;5;66;03m# constructor is also called from PandasMultiIndex)\u001b[39;00m\n\u001b[0;32m--> 582\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[43msafe_cast_to_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    584\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m index\u001b[38;5;241m.\u001b[39mname \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    585\u001b[0m         index\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m dim\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/indexes/multi.py:1193\u001b[0m, in \u001b[0;36mMultiIndex.copy\u001b[0;34m(self, names, deep, name)\u001b[0m\n\u001b[1;32m   1190\u001b[0m levels \u001b[38;5;241m=\u001b[39m levels \u001b[38;5;28;01mif\u001b[39;00m levels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlevels\n\u001b[1;32m   1191\u001b[0m codes \u001b[38;5;241m=\u001b[39m codes \u001b[38;5;28;01mif\u001b[39;00m codes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodes\n\u001b[0;32m-> 1193\u001b[0m new_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1194\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1196\u001b[0m \u001b[43m    \u001b[49m\u001b[43msortorder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msortorder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1197\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1200\u001b[0m new_index\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m   1201\u001b[0m new_index\u001b[38;5;241m.\u001b[39m_cache\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlevels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)  \u001b[38;5;66;03m# GH32669\u001b[39;00m\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/indexes/multi.py:339\u001b[0m, in \u001b[0;36mMultiIndex.__new__\u001b[0;34m(cls, levels, codes, sortorder, names, dtype, copy, name, verify_integrity)\u001b[0m\n\u001b[1;32m    336\u001b[0m result\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m# we've already validated levels and codes, so shortcut here\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_set_levels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    340\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_codes(codes, copy\u001b[38;5;241m=\u001b[39mcopy, validate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    342\u001b[0m result\u001b[38;5;241m.\u001b[39m_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(levels)\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/indexes/multi.py:827\u001b[0m, in \u001b[0;36mMultiIndex._set_levels\u001b[0;34m(self, levels, level, copy, validate, verify_integrity)\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of levels must match length of level.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 827\u001b[0m     new_levels \u001b[38;5;241m=\u001b[39m \u001b[43mFrozenList\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlev\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlevels\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     level_numbers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_level_number(lev) \u001b[38;5;28;01mfor\u001b[39;00m lev \u001b[38;5;129;01min\u001b[39;00m level]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/indexes/multi.py:828\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of levels must match length of level.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m level \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     new_levels \u001b[38;5;241m=\u001b[39m FrozenList(\n\u001b[0;32m--> 828\u001b[0m         \u001b[43mensure_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m lev \u001b[38;5;129;01min\u001b[39;00m levels\n\u001b[1;32m    829\u001b[0m     )\n\u001b[1;32m    830\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    831\u001b[0m     level_numbers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_level_number(lev) \u001b[38;5;28;01mfor\u001b[39;00m lev \u001b[38;5;129;01min\u001b[39;00m level]\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:764\u001b[0m, in \u001b[0;36mIndex._view\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_view\u001b[39m(\u001b[38;5;28mself\u001b[39m: _IndexT) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _IndexT:\n\u001b[1;32m    761\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;124;03m    fastpath to make a shallow copy, i.e. new object with same data.\u001b[39;00m\n\u001b[1;32m    763\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_simple_new\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_references\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    766\u001b[0m     result\u001b[38;5;241m.\u001b[39m_cache \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cache\n\u001b[1;32m    767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/pandas/core/indexes/base.py:640\u001b[0m, in \u001b[0;36mIndex._simple_new\u001b[0;34m(cls, values, name, refs)\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(dtype)\n\u001b[1;32m    627\u001b[0m \u001b[38;5;66;03m# NOTE for new Index creation:\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \n\u001b[1;32m    629\u001b[0m \u001b[38;5;66;03m# - _simple_new: It returns new Index with the same type as the caller.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    637\u001b[0m \n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# See each method's docstring.\u001b[39;00m\n\u001b[0;32m--> 640\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_simple_new\u001b[39m(\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[_IndexT], values: ArrayLike, name: Hashable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, refs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    643\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _IndexT:\n\u001b[1;32m    644\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    645\u001b[0m \u001b[38;5;124;03m    We require that we have a dtype compat for the values. If we are passed\u001b[39;00m\n\u001b[1;32m    646\u001b[0m \u001b[38;5;124;03m    a non-dtype compat, then coerce using the constructor.\u001b[39;00m\n\u001b[1;32m    647\u001b[0m \n\u001b[1;32m    648\u001b[0m \u001b[38;5;124;03m    Must be careful not to recurse.\u001b[39;00m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_data_cls), \u001b[38;5;28mtype\u001b[39m(values)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1695938557279
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}